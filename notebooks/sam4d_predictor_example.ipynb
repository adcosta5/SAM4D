{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Cross modal segmentation with SAM4D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {},
   "source": [
    "This notebook shows how to use SAM4D for interactive segmentation in camera and LiDAR streams. It will cover the following:\n",
    "\n",
    "- adding clicks (or box) on a frame in camera or LiDAR to get and refine _masklets_ (spatio-temporal masks)\n",
    "- propagating clicks (or box) to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire stream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d15921-ecd7-4c82-a1f5-ad8dbdc05bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import copy\n",
    "from sam4d.visualize import sam4d_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Building the SAM4D predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "from mmengine.registry import MODELS\n",
    "from sam4d.misc import _load_sam4d_label, concat_points, get_next_point, sample_box_points, _load_checkpoint\n",
    "\n",
    "\n",
    "def build_sam4d_predictor(\n",
    "        config_file,\n",
    "        ckpt_path=None,\n",
    "        device=\"cuda\",\n",
    "        mode=\"eval\",\n",
    "        cfg_options_extra={},\n",
    "        apply_postprocessing=True,\n",
    "        **kwargs,\n",
    "):\n",
    "    cfg_options = {\n",
    "        \"model.type\": \"SAM4DPredictor\",\n",
    "    }\n",
    "    if apply_postprocessing:\n",
    "        cfg_options_extra = cfg_options_extra.copy()\n",
    "        cfg_options_extra.update({\n",
    "            # dynamically fall back to multi-mask if the single mask is not stable\n",
    "            \"model.head.sam.sam_mask_decoder_extra_args.dynamic_multimask_via_stability\": True,\n",
    "            \"model.head.sam.sam_mask_decoder_extra_args.dynamic_multimask_stability_delta\": 0.05,\n",
    "            \"model.head.sam.sam_mask_decoder_extra_args.dynamic_multimask_stability_thresh\": 0.98,\n",
    "            # the sigmoid mask logits on interacted frames with clicks in the memory encoder so that the encoded masks are exactly as what users see from clicking\n",
    "            \"model.binarize_mask_from_pts_for_mem_enc\": False,  # to keep same as training\n",
    "            # fill small holes in the low-res masks up to `fill_hole_area` (before resizing them to the original video resolution)\n",
    "            \"model.fill_hole_area\": 8,\n",
    "        })\n",
    "\n",
    "    cfg_options.update(cfg_options_extra)\n",
    "\n",
    "    # os.environ['INFER_WITH_GT'] = '0'  # do not add gt in pipeline\n",
    "    cfg = Config.fromfile(config_file)\n",
    "    if cfg_options is not None:\n",
    "        cfg.merge_from_dict(cfg_options)\n",
    "\n",
    "    # model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "    model = MODELS.build(cfg.model)\n",
    "    model.set_data_pipeline(cfg.data.test.pipeline)\n",
    "    _load_checkpoint(model, ckpt_path)\n",
    "\n",
    "    model = model.to(device)\n",
    "    if mode == \"eval\":\n",
    "        model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57935cea-4017-4a3b-9a3e-42ab4d939beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam4d_checkpoint = \"../checkpoints/sam4d_hieraS_mink34w32.pth\"\n",
    "model_cfg = \"../configs/sam4d_hieraS_mink34w32.py\"\n",
    "\n",
    "predictor = build_sam4d_predictor(model_cfg, sam4d_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Select an example data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
   "metadata": {},
   "source": [
    "We use data in `../data/samples/waymo` for example, please refer to **Prepare Data** in README.md to get yourself data ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/samples/waymo\"\n",
    "seq_name = \"segment-10868756386479184868_3000_000_3020_000_with_camera_labels\"\n",
    "cam_name = \"FRONT\"\n",
    "\n",
    "with open(os.path.join(data_dir, 'meta_infos', f'{seq_name}.pkl'), 'rb') as f:\n",
    "    meta_info = pickle.load(f)\n",
    "\n",
    "img_paths, metas, lidar_paths = [], [], []\n",
    "for frame_info in meta_info['frames']:\n",
    "    img_path = os.path.join(data_dir, frame_info['cams_info'][cam_name]['data_path'])\n",
    "    img_paths.append(img_path)\n",
    "    camera_intrinsics = np.eye(4)\n",
    "    camera_intrinsics[:3, :3] = frame_info['cams_info'][cam_name]['camera_intrinsics'][:3, :3]\n",
    "    metas.append({'pose': frame_info['lidar2world'],\n",
    "                  'camera_intrinsics': camera_intrinsics,\n",
    "                  'camera2lidar': frame_info['cams_info'][cam_name]['camera2lidar']})\n",
    "    lidar_path = os.path.join(data_dir, frame_info['path']['pcd'])\n",
    "    lidar_paths.append(lidar_path)\n",
    "\n",
    "# take a look the first frame\n",
    "frame_idx = 0\n",
    "img = np.array(Image.open(img_paths[frame_idx]))\n",
    "pcd = np.load(lidar_paths[frame_idx])['data']\n",
    "\n",
    "frame_0 = sam4d_viz(img, pcd, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(frame_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {},
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {},
   "source": [
    "SAM4D requires stateful inference for interactive stream segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the camera and LiDAR frames in `img_paths` and `pts_paths`, and stores their data in `inference_state` (as shown in the progress bar below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(img_paths=img_paths, pts_paths=lidar_paths, metas=metas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1f3f6-d74d-4016-934c-8d2a14d1a543",
   "metadata": {},
   "source": [
    "### Example 1: Segment & track one object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `reset_state` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### Step 1: Add a first click on a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c7749-b523-4691-aad0-7558c5d1d68c",
   "metadata": {},
   "source": [
    "To get started, let's try to segment the car on the front.\n",
    "\n",
    "Here we make a **positive click** at (u, v) = (1000, 1000) in camera with label `1`, by sending their coordinates and labels into the `add_new_points_or_box` API.\n",
    "\n",
    "Note: label `1` indicates a *positive click (to add a region)* while label `0` indicates a *negative click (to remove a region)*. In the images, we use green and red pentagrams to represent positive and negative prompts respectively, while in the point clouds we employ green and red spheres for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749bab-0f36-4173-bf8d-0c20cd5214b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Here point_prompts should be a dict\n",
    "point_prompts = {\n",
    "    'img': {\n",
    "        'point_coords': torch.tensor([1000, 1000]).float().reshape(1, 1, 2), # (n_obj, n_prompt, uv)  \n",
    "        'point_labels': torch.tensor([1]).int().reshape(1, 1),\n",
    "    },\n",
    "}\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=copy.deepcopy(point_prompts),\n",
    "    box_prompts=None,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "img = np.array(Image.open(os.path.join(img_paths[ann_frame_idx])))\n",
    "pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "viz_ret = sam4d_viz(img, pcd, out_mask_logits, out_obj_ids, point_prompts=point_prompts, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9cb64-1328-4306-9669-a904d7deae97",
   "metadata": {},
   "source": [
    "In addition to prompting on images, we can also perform prompting on point clouds. For example, selecting a single point from the point cloud of the vehicle ahead, such as (x, y, z) = (8.9062, 0.1504, 1.4072)\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Tips: you can use plotly to interactively pick the point cloud coordinates.\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "o3d_pcd = o3d.geometry.PointCloud()\n",
    "o3d_pcd.points = o3d.utility.Vector3dVector(pcd)\n",
    "\n",
    "hover_texts = [f\"X: {p[0]:.4f}<br>Y: {p[1]:.3f}<br>Z: {p[2]:.3f}\" for p in pcd]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Scatter3d(\n",
    "        x=pcd[:, 0],\n",
    "        y=pcd[:, 1],\n",
    "        z=pcd[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color='lightblue', opacity=0.8),\n",
    "        hoverinfo='text',\n",
    "        text=hover_texts\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
    "        aspectmode='data'\n",
    "    ),\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51a8c4-8ffa-4636-b67b-b48054977eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state) # dont forget to reset\n",
    "\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Here point_prompts should be a dict\n",
    "point_prompts = {\n",
    "    'pts': {\n",
    "        'point_coords': torch.tensor([8.9062, 0.1504, 1.4072]).float().reshape(1, 1, 3), # (n_obj, n_prompt, xyz)  \n",
    "        'point_labels': torch.tensor([1]).int().reshape(1, 1),\n",
    "    },\n",
    "}\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=copy.deepcopy(point_prompts),\n",
    "    box_prompts=None,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "viz_ret = sam4d_viz(img, pcd, out_mask_logits, out_obj_ids, point_prompts=point_prompts, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89457875-93fa-40ed-b6dc-4e1c971a27f9",
   "metadata": {},
   "source": [
    "#### Step 2: Add a second click (support cross-modal) to refine the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75eb21b-1413-452c-827b-a04093c30c78",
   "metadata": {},
   "source": [
    "Hmm, it seems that the car's roof isn't well segmented, we can refine it with a second positive click on image.\n",
    "\n",
    "Here we make a **second positive click** at (u, v) = (830, 741) with label `1` to expand the mask.\n",
    "\n",
    "Note: we need to send **all the clicks and their labels** (i.e. not just the last click) when calling `add_new_points_or_box`.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Tips: you may use plotly to interactively pick the pixel coordinates.\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "from PIL import Image\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(go.Image(z=viz_ret))\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab3ec7-2537-4158-bf98-3d0977d8908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd positive click at (u, v) = (830, 741) to refine the mask\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "point_prompts = {\n",
    "    'pts': {\n",
    "        'point_coords': torch.tensor([8.9062, 0.1504, 1.4072]).float().reshape(1, 1, 3), # (n_obj, n_prompt, xyz)  \n",
    "        'point_labels': torch.tensor([1]).int().reshape(1, 1),\n",
    "    },\n",
    "    'img': {\n",
    "        'point_coords': torch.tensor([831, 732]).float().reshape(1, 1, 2), # (n_obj, n_prompt, uv)  \n",
    "        'point_labels': torch.tensor([1]).int().reshape(1, 1),\n",
    "    },\n",
    "}\n",
    "\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=copy.deepcopy(point_prompts),\n",
    "    box_prompts=None,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "img = np.array(Image.open(os.path.join(img_paths[ann_frame_idx])))\n",
    "pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "viz_ret = sam4d_viz(img, pcd, out_mask_logits, out_obj_ids, point_prompts=point_prompts, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52015ac-1b7b-4c59-bca3-c2b28484cf46",
   "metadata": {},
   "source": [
    "#### Step 3: Propagate the prompts to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}   # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        'instance_ids': out_obj_ids,\n",
    "        'mask_logits_dict': out_mask_logits,\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "for out_frame_idx in range(0, inference_state['num_frames'], vis_frame_stride):\n",
    "    cur_img = np.array(Image.open(os.path.join(img_paths[out_frame_idx])))\n",
    "    cur_pcd = np.load(lidar_paths[out_frame_idx])['data']\n",
    "    viz_ret = sam4d_viz(cur_img, cur_pcd, **video_segments[out_frame_idx], title=f'frame {out_frame_idx}')\n",
    "    display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e801b70-72df-4a72-b3fe-84f145e5e3f6",
   "metadata": {},
   "source": [
    "#### Step 4: Add new prompts to further refine the masklet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478958ab-29b4-4a75-bba4-adb1b03d0a2b",
   "metadata": {},
   "source": [
    "It appears that in the output masklet above, there are some small imperfections in boundary details on frame 150 of LiDAR.\n",
    "\n",
    "With SAM4D we can fix the model predictions interactively. We can add a **negative click** in LiDAR-modal at (x, y, z) = (8.2500, -2.771, 0.0008) on this frame with label `0` to refine the masklet. Here we call the `add_new_points_or_box` API with a different `frame_idx` argument to indicate the frame index we want to refine.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Tips: you can use plotly to interactively pick the point cloud coordinates.\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "ann_frame_idx = 150\n",
    "tmp_pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "tmp_mask = video_segments[ann_frame_idx]['mask_logits_dict']['pts'].cpu().numpy() > 0.0\n",
    "tmp_pcd = tmp_pcd[tmp_mask[0, 0, :, 0]]\n",
    "o3d_pcd = o3d.geometry.PointCloud()\n",
    "o3d_pcd.points = o3d.utility.Vector3dVector(tmp_pcd)\n",
    "\n",
    "\n",
    "hover_texts = [f\"X: {p[0]:.4f}<br>Y: {p[1]:.3f}<br>Z: {p[2]:.3f}\" for p in tmp_pcd]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Scatter3d(\n",
    "        x=tmp_pcd[:, 0],\n",
    "        y=tmp_pcd[:, 1],\n",
    "        z=tmp_pcd[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color='lightblue', opacity=0.8),\n",
    "        hoverinfo='text',\n",
    "        text=hover_texts\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
    "        aspectmode='data'\n",
    "    ),\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a572ea9-5b7e-479c-b30c-93c38b121131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ann_frame_idx = 150  # further refine some details on this frame\n",
    "ann_obj_id = 1  # give a unique id to the object we interact with (it can be any integers)\n",
    "\n",
    "# show the segment before further refinement\n",
    "cur_img = np.array(Image.open(os.path.join(img_paths[ann_frame_idx])))\n",
    "cur_pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "viz_ret = sam4d_viz(cur_img, cur_pcd, **video_segments[ann_frame_idx], title=f'frame {ann_frame_idx} -- before refinement')\n",
    "display(Image.fromarray(viz_ret))\n",
    "\n",
    "# Let's add a negative click in LiDAR-modal on this frame to refine the segment\n",
    "point_prompts = {\n",
    "    # 'img': {\n",
    "    #     'point_coords': torch.tensor([300, 300, 800, 800, 1000, 1000]).float().reshape(1, 3, 2), # (n_obj, n_prompt, xyz)  \n",
    "    #     'point_labels': torch.tensor([1, 0, 0]).int().reshape(1, 3),\n",
    "    # },\n",
    "    'pts': {\n",
    "        'point_coords': torch.tensor([8.2500, -2.771, 0.0008]).float().reshape(1, 1, 3), # (n_obj, n_prompt, xyz)  \n",
    "        # 'point_coords': torch.tensor([8.8906, -1.646, 0.009]).float().reshape(1, 1, 3), # (n_obj, n_prompt, xyz)  \n",
    "        'point_labels': torch.tensor([0]).int().reshape(1, 1),\n",
    "    },\n",
    "}\n",
    "# inference_state[\"frames_tracked_per_obj\"][0].pop(ann_frame_idx, None) # clear mem\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=copy.deepcopy(point_prompts),\n",
    "    box_prompts=None,\n",
    ")\n",
    "\n",
    "# show the segment after the further refinement\n",
    "viz_ret = sam4d_viz(cur_img, cur_pcd, out_mask_logits, out_obj_ids, point_prompts=point_prompts, title=f'frame {ann_frame_idx} -- after refinement')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3950a-acf1-435c-bd64-94297267b5e9",
   "metadata": {},
   "source": [
    "#### Step 5: Propagate the prompts (again) to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1954ecf-c2ec-4f9c-8d10-c4f527a10cd2",
   "metadata": {},
   "source": [
    "Let's get an updated masklet for the entire video. Here we call `propagate_in_video` again to propagate all the prompts after adding the new refinement click above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa96690-4a38-4a24-aa17-fd2f4db0e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}   # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        'instance_ids': out_obj_ids,\n",
    "        'mask_logits_dict': out_mask_logits,\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "for out_frame_idx in range(0, inference_state['num_frames'], vis_frame_stride):\n",
    "    cur_img = np.array(Image.open(os.path.join(img_paths[out_frame_idx])))\n",
    "    cur_pcd = np.load(lidar_paths[out_frame_idx])['data']\n",
    "    viz_ret = sam4d_viz(cur_img, cur_pcd, **video_segments[out_frame_idx], title=f'frame {out_frame_idx}')\n",
    "    display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5893a-6a96-475e-9430-4622887e83aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e682c0-8171-415d-b51f-2f9e67963641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2502bb5a-3e1f-43d0-9f58-33f8676fff0d",
   "metadata": {},
   "source": [
    "### Example 2: Segment an object using box prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d26c8-0432-48c6-997e-4a3b77bb5f6d",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe9183-abbb-4283-b0cb-d24f3d7beb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6eae9-0f4c-434f-8089-a46c9ca59da5",
   "metadata": {},
   "source": [
    "In addition to using clicks as inputs, SAM 2 also supports segmenting and tracking objects in a video via **bounding boxes**.\n",
    "\n",
    "In the example below, we segment the child on the right using a **box prompt** of (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) on frame 0 as input into the `add_new_points_or_box` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfb273-4e14-495b-bd89-87a8baf52ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a box on image at (x_min, y_min, x_max, y_max) = (495, 644, 762, 876) to get started\n",
    "box_prompts = {\n",
    "    'img': {\n",
    "        'point_coords': torch.tensor([495, 644, 762, 876]).float().reshape(1, 2, 2), # (n_obj, n_prompt, uv)  \n",
    "        'point_labels': torch.tensor([2, 3]).int().reshape(1, 2),\n",
    "    },\n",
    "}\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=None,\n",
    "    box_prompts=copy.deepcopy(box_prompts),\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "img = np.array(Image.open(os.path.join(img_paths[ann_frame_idx])))\n",
    "pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "viz_ret = sam4d_viz(img, pcd, out_mask_logits, out_obj_ids, box_prompts=box_prompts, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73128cd6-dbfa-49f7-8d79-1a8e19835f7f",
   "metadata": {},
   "source": [
    "Then, to get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b0dcc-1599-4a5e-bc2e-1ccc2a89c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}   # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        'instance_ids': out_obj_ids,\n",
    "        'mask_logits_dict': out_mask_logits,\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "for out_frame_idx in range(0, inference_state['num_frames'], vis_frame_stride):\n",
    "    cur_img = np.array(Image.open(os.path.join(img_paths[out_frame_idx])))\n",
    "    cur_pcd = np.load(lidar_paths[out_frame_idx])['data']\n",
    "    viz_ret = sam4d_viz(cur_img, cur_pcd, **video_segments[out_frame_idx], title=f'frame {out_frame_idx}')\n",
    "    display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023f91f-0cc5-4980-ae8e-a13c5749112b",
   "metadata": {},
   "source": [
    "Note that in addition to clicks or boxes, SAM4D also supports directly using a **mask prompt** as input via the `add_new_mask` method in the `SAM4DPredictor` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68",
   "metadata": {},
   "source": [
    "### Example 3: Segment multiple objects simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6c04c-3072-4876-b394-879321a48c4a",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b874c8-9f39-42d3-a667-54a0bd696410",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3f7e6-4821-468c-84e4-f3a0435c9149",
   "metadata": {},
   "source": [
    "#### Step 1: Add two objects on a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95158714-86d7-48a9-8365-b213f97cc9ca",
   "metadata": {},
   "source": [
    "SAM4D can also segment and track two or more objects at the same time. One way, of course, is to do them one by one. However, it would be more efficient to batch them together (e.g. so that we can share the image features between objects to reduce computation costs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9ac57-b14a-4237-828d-927e422c518b",
   "metadata": {},
   "source": [
    "Add the first object (the left child's shirt) with a **positive click** at (x, y) = (200, 300) on frame 0.\n",
    "\n",
    "We assign it to object id `2` (it can be arbitrary integers, and only needs to be unique for each object to track), which is passed to the `add_new_points_or_box` API to distinguish the object we are clicking upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c051cbc-980b-46c9-8242-75a164562e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure(go.Image(z=Image.open(os.path.join(img_paths[0]))))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033dc995-8659-45ea-bce1-f32e47b57a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Here point_prompts should be a dict\n",
    "point_prompts = {\n",
    "    'img': {\n",
    "        'point_coords': torch.tensor([343, 783]).float().reshape(1, 1, 2), # (n_obj, n_prompt, uv)  \n",
    "        'point_labels': torch.tensor([1]).int().reshape(1, 1),\n",
    "    },\n",
    "}\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=copy.deepcopy(point_prompts),\n",
    "    box_prompts=None,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "img = np.array(Image.open(os.path.join(img_paths[ann_frame_idx])))\n",
    "pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "viz_ret = sam4d_viz(img, pcd, out_mask_logits, out_obj_ids, point_prompts=point_prompts, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd51b-e1e2-4c36-99ec-1d9a1b49b0cd",
   "metadata": {},
   "source": [
    "Hmm, this time we just want to select one black car, but the model predicts two cars in LiDAR modal.. Let's refine the prediction with a **negative click** at (x, y, z) = (21.6758, 9.584, 0.949) in point clouds.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Tips: you can use plotly to interactively pick the point cloud coordinates.\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "ann_frame_idx = 0\n",
    "tmp_pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "tmp_mask = out_mask_logits['pts'].cpu().numpy() > 0.0\n",
    "tmp_pcd = tmp_pcd[tmp_mask[0, 0, :, 0]]\n",
    "o3d_pcd = o3d.geometry.PointCloud()\n",
    "o3d_pcd.points = o3d.utility.Vector3dVector(tmp_pcd)\n",
    "\n",
    "\n",
    "hover_texts = [f\"X: {p[0]:.4f}<br>Y: {p[1]:.3f}<br>Z: {p[2]:.3f}\" for p in tmp_pcd]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Scatter3d(\n",
    "        x=tmp_pcd[:, 0],\n",
    "        y=tmp_pcd[:, 1],\n",
    "        z=tmp_pcd[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color='lightblue', opacity=0.8),\n",
    "        hoverinfo='text',\n",
    "        text=hover_texts\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
    "        aspectmode='data'\n",
    "    ),\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85333b3b-875f-42ee-a5a1-7117bba766a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine the first object\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Here point_prompts should be a dict\n",
    "point_prompts = {\n",
    "    'img': {\n",
    "        'point_coords': torch.tensor([343, 783]).float().reshape(1, 1, 2), # (n_obj, n_prompt, uv)  \n",
    "        'point_labels': torch.tensor([1]).int().reshape(1, 1),\n",
    "    },\n",
    "    'pts': {\n",
    "        'point_coords': torch.tensor([21.6758, 9.584, 0.949]).float().reshape(1, 1, 3), # (n_obj, n_prompt, xyz)  \n",
    "        'point_labels': torch.tensor([0]).int().reshape(1, 1),\n",
    "    },\n",
    "}\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=copy.deepcopy(point_prompts),\n",
    "    box_prompts=None,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "img = np.array(Image.open(os.path.join(img_paths[ann_frame_idx])))\n",
    "pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "viz_ret = sam4d_viz(img, pcd, out_mask_logits, out_obj_ids, point_prompts=point_prompts, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194718c1-734d-446c-a3ef-361057de2f31",
   "metadata": {},
   "source": [
    "After the 2nd negative click, now we get the left child's shirt as our first object.\n",
    "\n",
    "Let's move on to the second object (the right child's shirt) with a positive click at (x, y) = (400, 150) on frame 0. Here we assign object id `3` to this second object (it can be arbitrary integers, and only needs to be unique for each object to track).\n",
    "\n",
    "Note: when there are multiple objects, the `add_new_points_or_box` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca1bde-62a4-40e6-98e4-15606441e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 3  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Here point_prompts should be a dict\n",
    "point_prompts = {\n",
    "    'img': {\n",
    "        'point_coords': torch.tensor([1120, 704]).float().reshape(1, 1, 2), # (n_obj, n_prompt, uv)  \n",
    "        'point_labels': torch.tensor([1]).int().reshape(1, 1),\n",
    "    },\n",
    "}\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    point_prompts=copy.deepcopy(point_prompts),\n",
    "    box_prompts=None,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "img = np.array(Image.open(os.path.join(img_paths[ann_frame_idx])))\n",
    "pcd = np.load(lidar_paths[ann_frame_idx])['data']\n",
    "viz_ret = sam4d_viz(img, pcd, out_mask_logits, out_obj_ids, point_prompts=point_prompts, title=f'frame {frame_idx}')\n",
    "display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7add8-d577-4597-ae2f-654b8c7b05e0",
   "metadata": {},
   "source": [
    "This time the model predicts the mask of the shirt we want to track in just one click. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448733b8-ea8b-4078-995f-b676c3b558ba",
   "metadata": {},
   "source": [
    "#### Step 2: Propagate the prompts to get masklets across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {},
   "source": [
    "Now, we propagate the prompts for both objects to get their masklets throughout the video.\n",
    "\n",
    "Note: when there are multiple objects, the `propagate_in_video` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}   # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        'instance_ids': out_obj_ids,\n",
    "        'mask_logits_dict': out_mask_logits,\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "for out_frame_idx in range(0, inference_state['num_frames'], vis_frame_stride):\n",
    "    cur_img = np.array(Image.open(os.path.join(img_paths[out_frame_idx])))\n",
    "    cur_pcd = np.load(lidar_paths[out_frame_idx])['data']\n",
    "    viz_ret = sam4d_viz(cur_img, cur_pcd, **video_segments[out_frame_idx], title=f'frame {out_frame_idx}')\n",
    "    display(Image.fromarray(viz_ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd986b-2aa9-4b60-b372-4071882ec536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246ad4e-c5b4-4672-bce5-9c2bde95f738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
